# WaveGrad
Implementation (PyTorch) of Google Brain's WaveGrad vocoder (paper: https://arxiv.org/pdf/2009.00713.pdf).

#### Status (in active development)

* **Stable versions: 25-, 50- and 1000-iteration models with linear betas.**
* Conducting experiments on 6-iterations training.
* Uploaded demo sample generated by current 50-iteration model.
* **Estimated real-time factor (RTF)** for 6-, 25-, 50- and 1000-iteration models (see the table below). 50- and lower-iteration models have faster than real-time inference speed. 6-iteration model potentially faster than the model reported in the paper.
* Preparing pretrained checkpoints.
* Resolved positional encoding issue from previous model version. Now model can sample much faster from the whole mel-spectrogram with higher quality.

#### Run-time factor

|          Model         |  Stable  | RTF (NVIDIA RTX 2080 Ti), 22Khz |
|------------------------|----------|---------------------------------|
| 1000 iterations Base   |   True   |          9.59 ± 0.357           |
|   50 iterations Base   |   True   |          0.45 ± 0.021           |
|   25 iterations Base   |   True   |          0.22 ± 0.011           |
|    6 iterations Base   |   None   |          0.04 ± 0.005           |

## About

WaveGrad is a conditional model for waveform generation through estimating gradients of the data density. The main concept of vocoder is its relatedness to diffusion models based on Langevin dynamics and score matching frameworks. w.r.t. Langevin dynamics WaveGrad achieves super-fast convergence (6 iterations reported in the paper).

## Setup

1. Clone this repo:

```bash
git clone https://github.com/ivanvovk/WaveGrad.git
cd WaveGrad
```

2. Install requirements `pip install -r requirements.txt`

## Train your own model

1. Make filelists of your data like ones included into `filelists` folder.
2. Setup a configuration in `configs` folder.
3. Change config path in `train.sh` and run the script by `sh train.sh`.
4. To track training process run tensorboard by `tensorboard --logdir=logs/YOUR_LOG_FOLDER`.

## Inference, generated audios and pretrained checkpoints

#### Inference

In order to make inference of your model follow instructions provided in Jupyter Notebook [`notebooks/inference.ipynb`](notebooks/inference.ipynb). Also there is the code to estimate RTF in your runtime environment.

#### Generated audios

Current generated audios are provided in [`generated_samples`](generated_samples/) folder.

#### Pretrained checkpoints

In progress.

## Details, issues and comments

* For Langevin dynamics computation [`Denoising Diffusion Probabilistic Models` repository](https://github.com/hojonathanho/diffusion) has been adopted.
* **Model training succesfully runs on a single 12GB GPU**. Batch size is modified compared to the paper (256 -> 48, authors trained their model on TPU).
* At some point training might start to behave very weird and crazy (loss explodes), so I introduced learning rate scheduling and gradient clipping.
* Prefer using standard noise schedulles for 25-, 50- and 1000-iteration models from defualt configs.
* 6-iteration training is unstable currently.
* Hop length of your STFT should always be equal 300. Other cases are not supported yet.

## References

* [WaveGrad: Estimating Gradients for Waveform Generation](https://arxiv.org/pdf/2009.00713.pdf)
* [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf)
* [Denoising Diffusion Probabilistic Models repository](https://github.com/hojonathanho/diffusion), from which diffusion calculations have been adopted
